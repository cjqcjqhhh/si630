{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef751bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# README:\n",
    "# This file mainly used for tokenize variation\n",
    "# In file SI630_Assignment1_junqich.ipynb (main file), I use only better_tokenize()\n",
    "# In this file tokenize_variation.ipynb, I use only tokenize()\n",
    "# And save the result for task3.4, which is read by main file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e645531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from scipy import sparse\n",
    "from scipy.stats import uniform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9748e79a",
   "metadata": {},
   "source": [
    "# Part 1: Representing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2298909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party_affiliation</th>\n",
       "      <th>email_text</th>\n",
       "      <th>uid_email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱...</td>\n",
       "      <td>3083493a6b205eabd8d0f1e7772db09b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>This new report needs your attention now. \\n  ...</td>\n",
       "      <td>3a583e28c820e1fac8902e4df0ef50e0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>&lt;PARTY&gt; &lt;ORG&gt; &lt;ORG&gt; &lt;ORG&gt; \\n &lt;&gt; NEWS \\n  &lt;PERS...</td>\n",
       "      <td>044219f46cca419d1d95242dfe036c15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>Chip in today to &lt;GPE&gt; to our virtual grassroo...</td>\n",
       "      <td>68059dd1d93d0cbf456763822d1ab680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>I ’m really sorry to bother you but I ’m not o...</td>\n",
       "      <td>9f41a878e2839dc013546e615da83efa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  party_affiliation                                         email_text  \\\n",
       "0  Democratic Party  ⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱⋱...   \n",
       "1  Democratic Party  This new report needs your attention now. \\n  ...   \n",
       "2  Democratic Party  <PARTY> <ORG> <ORG> <ORG> \\n <> NEWS \\n  <PERS...   \n",
       "3  Democratic Party  Chip in today to <GPE> to our virtual grassroo...   \n",
       "4  Democratic Party  I ’m really sorry to bother you but I ’m not o...   \n",
       "\n",
       "                          uid_email  \n",
       "0  3083493a6b205eabd8d0f1e7772db09b  \n",
       "1  3a583e28c820e1fac8902e4df0ef50e0  \n",
       "2  044219f46cca419d1d95242dfe036c15  \n",
       "3  68059dd1d93d0cbf456763822d1ab680  \n",
       "4  9f41a878e2839dc013546e615da83efa  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f09f5733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party_affiliation</th>\n",
       "      <th>email_text</th>\n",
       "      <th>uid_email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>&lt;&gt; &lt;&gt;, \\n &lt;DATE&gt;, &lt;&gt; &lt;PERSON&gt; &lt;PERSON&gt; suspend...</td>\n",
       "      <td>64241625785edfde727dd84c08e5cda2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>We simply could not run our campaign without o...</td>\n",
       "      <td>d005af10b61a2565704c237fd506b5e9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>I have to give &lt;&gt; &lt;&gt; &lt;PERSON&gt; and the team an ...</td>\n",
       "      <td>75088c6211cca345172d18aab778b93c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>Here ’s your challenge: Can we raise $ 10,000 ...</td>\n",
       "      <td>b56badd20bd35485b7197587333283d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>This week, I and more than 30 of my colleagues...</td>\n",
       "      <td>f60dc5576465f00970e35e36e57e9f1e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  party_affiliation                                         email_text  \\\n",
       "0  Democratic Party  <> <>, \\n <DATE>, <> <PERSON> <PERSON> suspend...   \n",
       "1  Democratic Party  We simply could not run our campaign without o...   \n",
       "2  Democratic Party  I have to give <> <> <PERSON> and the team an ...   \n",
       "3  Democratic Party  Here ’s your challenge: Can we raise $ 10,000 ...   \n",
       "4  Democratic Party  This week, I and more than 30 of my colleagues...   \n",
       "\n",
       "                          uid_email  \n",
       "0  64241625785edfde727dd84c08e5cda2  \n",
       "1  d005af10b61a2565704c237fd506b5e9  \n",
       "2  75088c6211cca345172d18aab778b93c  \n",
       "3  b56badd20bd35485b7197587333283d1  \n",
       "4  f60dc5576465f00970e35e36e57e9f1e  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev = pd.read_csv(\"dev.csv\")\n",
    "dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0406039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_text</th>\n",
       "      <th>uid_email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20,000 &lt;GPE&gt; &lt;GPE&gt; signatures needed \\n 20,000...</td>\n",
       "      <td>5dfbe09ce5b500dd3dcb9f93c8fb185f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We 've worked way too hard and given way too m...</td>\n",
       "      <td>07e48f4183b98420a18503791fb412f6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you use your 800%-MATCH \\n  &lt;&gt; \\n &lt;ORG&gt; &lt;OR...</td>\n",
       "      <td>b58c8607d96a414db0e9cc10108c35f5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;PERSON&gt;, a proposal for Vote- by- &lt;&gt; threaten...</td>\n",
       "      <td>3901d9539d69ada89e5c82e2f1ca950d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The &lt;ORG&gt; &lt;ORG&gt; &lt;ORG&gt; is under immediate threa...</td>\n",
       "      <td>6a3e8e6f31381e84a34571deee0f1238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          email_text  \\\n",
       "0  20,000 <GPE> <GPE> signatures needed \\n 20,000...   \n",
       "1  We 've worked way too hard and given way too m...   \n",
       "2  If you use your 800%-MATCH \\n  <> \\n <ORG> <OR...   \n",
       "3  <PERSON>, a proposal for Vote- by- <> threaten...   \n",
       "4  The <ORG> <ORG> <ORG> is under immediate threa...   \n",
       "\n",
       "                          uid_email  \n",
       "0  5dfbe09ce5b500dd3dcb9f93c8fb185f  \n",
       "1  07e48f4183b98420a18503791fb412f6  \n",
       "2  b58c8607d96a414db0e9cc10108c35f5  \n",
       "3  3901d9539d69ada89e5c82e2f1ca950d  \n",
       "4  6a3e8e6f31381e84a34571deee0f1238  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c32e2b1",
   "metadata": {},
   "source": [
    "## Task 1.1: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7e05ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = '''!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~—'''\n",
    "stopwords = {'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "510fe706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_str):\n",
    "    token = input_str.split()\n",
    "    return token\n",
    "\n",
    "def better_tokenize(input_str):\n",
    "    # manipulation\n",
    "    temp_str = input_str.replace('\\n', '')\n",
    "    temp_str = re.sub(r\"\\<[^<>]*\\>\", '', temp_str) # remove hidden names inside <>\n",
    "    for punc in punctuations:\n",
    "        temp_str = temp_str.replace(punc, '') # remove punctuations\n",
    "    token = temp_str.lower().split() # turn into lower cases\n",
    "    token = [t for t in token if not t in stopwords] # remove stopwords\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872bd031",
   "metadata": {},
   "source": [
    "## Task 1.2: Building the Term-Document Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23fdeed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terms in term_dict is 29060.\n"
     ]
    }
   ],
   "source": [
    "min_tf = 10\n",
    "term_dict = defaultdict(int) # mapping from term to total tf\n",
    "all_term_dict = defaultdict(int)\n",
    "\n",
    "# set up a term dictionary for all\n",
    "for index, row in train.iterrows():\n",
    "    temp = Counter(tokenize(row[\"email_text\"])) # tf in one doc\n",
    "    for term, freq in temp.items():\n",
    "        all_term_dict[term] += freq\n",
    "    \n",
    "# remove terms less then minimum word frequency \n",
    "for term, freq in all_term_dict.items():\n",
    "    if freq >= min_tf:\n",
    "        term_dict[term] = freq\n",
    "        \n",
    "print(f\"Number of terms in term_dict is {len(term_dict)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb4792de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the COO sparse matrix\n",
    "# DO NOT USE! TOO SLOW!\n",
    "\n",
    "# doc_id = np.array([]) # doc dimension D\n",
    "# term_id = np.array([]) # vocabulary dimension V\n",
    "# doc_tf = np.array([]) # term df in a doc\n",
    "# for index, row in tqdm(train.iterrows(), total = train.shape[0]):\n",
    "#     temp = Counter(tokenize(row[\"email_text\"])) # tf in one doc\n",
    "#     for term, freq in temp.items():\n",
    "#         if term in term_id_dict.keys():\n",
    "#             doc_id = np.append(doc_id, index)\n",
    "#             term_id = np.append(term_id, term_id_dict[term])\n",
    "#             doc_tf = np.append(doc_tf, freq)\n",
    "        \n",
    "# term_doc_mat = sparse.coo_matrix((doc_tf, (doc_id, term_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4594181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the CSR sparse matrix\n",
    "def CSRMatrixGeneration(docs, vocabulary = {}, term_dict = term_dict):\n",
    "    # INPUT: docs is a vector length D with tokens of each doc\n",
    "    # OUTPUT: a CSR term freq matrix length V * D\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    \n",
    "    print(\"Generating CSR sparse matrix...\")\n",
    "    for doc in tqdm(docs):\n",
    "        for term in doc:\n",
    "            if term in term_dict.keys(): # remove term with tf < 10\n",
    "                index = vocabulary.setdefault(term, len(vocabulary))\n",
    "                indices.append(index)\n",
    "                data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "\n",
    "    csr_mat = sparse.csr_matrix((data, indices, indptr), dtype=int)\n",
    "    return csr_mat, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "841a1258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating doc list for train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 59999/59999 [00:03<00:00, 17408.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating CSR sparse matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 59999/59999 [00:06<00:00, 9300.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate doc list\n",
    "docs = []\n",
    "print(\"Generating doc list for train...\")\n",
    "for index, row in tqdm(train.iterrows(), total = train.shape[0]):\n",
    "    docs.append(tokenize(row[\"email_text\"]))\n",
    "    \n",
    "train_mat, train_vocabulary = CSRMatrixGeneration(docs)\n",
    "train_mat = sparse.hstack([train_mat, np.ones(len(train_mat.toarray()))[:,None]]) # add the bias column\n",
    "train_mat = sparse.csr_matrix(train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad0da3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the constructed term-document matrix is (D, V) = (59999, 29061)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the constructed term-document matrix is (D, V) = {train_mat.toarray().shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c8e954",
   "metadata": {},
   "source": [
    "# Part 2: Logistic Regression in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71dd163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    # Input: an np.array\n",
    "    # Output: an np.array\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "def log_likelihood(X, y, beta):\n",
    "    # INPUT: y, beta are 1-d np.array length V\n",
    "    # INPUT: X is a sprase matrix length doc number D * length V\n",
    "    # OUTPUT: np.array log-likelihood length V\n",
    "    sum = 0\n",
    "    n = len(X[0].toarray().flatten()) # vocabulary (feature) length\n",
    "    for i in range(n):\n",
    "        sum += y[i] * np.dot(beta, X[i].toarray().flatten()) - \\\n",
    "                np.log10(1 + np.exp(np.dot(beta, X[i].toarray().flatten())))\n",
    "    \n",
    "    return sum\n",
    "\n",
    "def compute_gradient(x, y, beta):\n",
    "    # INPUT: beta is 1-d np.array length V\n",
    "    # INPUT: y is 1-d np.array length 1\n",
    "    # INPUT: x is a 1-d np.array length V\n",
    "    # OUTPUT: 1-d np.array of gradient length V\n",
    "    return np.dot((sigmoid(np.dot(beta, x)) - y), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0311764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, learning_rate = 5e-5, num_step = 1000, is_plot = False):\n",
    "    # INPUT: X is a sparse matrix length doc number D * length V\n",
    "    # INPUT: y is a 1-d np.array length V\n",
    "    # OUTPUT: a trained parameter beta length V\n",
    "    n = len(X[0].toarray().flatten()) # vocabulary (feature) length\n",
    "    beta = np.zeros(n) # init beta\n",
    "    prev_ll = log_likelihood(X, y, beta) # for previous log-likelihood, recorded per 100 steps\n",
    "    if is_plot:\n",
    "        step_list = []\n",
    "        beta_list = [] # for log likelihood plot per 100 steps\n",
    "    \n",
    "    print(\"Starting Logistic Regression to find the parameter vector beta...\")\n",
    "    for step_count in tqdm(range(num_step)):\n",
    "        beta = beta - learning_rate * compute_gradient(X[step_count % n].toarray().flatten(), \n",
    "                                                       y[step_count % n], \n",
    "                                                       beta)\n",
    "        if step_count % 100 == 0:\n",
    "            if is_plot:\n",
    "                step_list.append(step_count)\n",
    "                beta_list.append(log_likelihood(X, y, beta))\n",
    "            \n",
    "#             curr_ll = log_likelihood(X, y, beta)\n",
    "#             if abs(curr_ll - prev_ll) < 1e-5:\n",
    "#                 print(\"The hyperparameter has converged. Early stop.\")\n",
    "#                 break\n",
    "#             else:\n",
    "#                 prev_ll = curr_ll\n",
    "        \n",
    "    if is_plot:\n",
    "        plt.plot(step_list, beta_list)\n",
    "        \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f57b71af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, beta, vocab_dict = train_vocabulary):\n",
    "    x_pred = np.zeros(len(vocab_dict) + 1)\n",
    "    \n",
    "    term_dict = Counter(tokenize(text))\n",
    "    for term, freq in term_dict.items():\n",
    "        if term in vocab_dict.keys():\n",
    "            x_pred[vocab_dict[term]] = freq\n",
    "    x_pred[-1] = 1 # bias\n",
    "    \n",
    "    y_pred = sigmoid(np.dot(beta, x_pred))\n",
    "    if y_pred < 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947191ee",
   "metadata": {},
   "source": [
    "## Task 2.1: Plot log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2e9c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'Democratic Party': 0, 'Republican Party': 1}\n",
    "y_train = np.array([label_dict[p] for p in train[\"party_affiliation\"]])\n",
    "\n",
    "# beta = logistic_regression(X = train_mat,\n",
    "#                            y = y_train, \n",
    "#                            is_plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d1d18e",
   "metadata": {},
   "source": [
    "## Task 2.2: Make prediction on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3343a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train the beta\n",
    "# beta = logistic_regression(X = train_mat,\n",
    "#                            y = y_train, \n",
    "#                            learning_rate = 5e-5, \n",
    "#                            num_step = 600000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "138b7152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "# y_test = [label_dict[p] for p in dev[\"party_affiliation\"]]\n",
    "# y_pred = []\n",
    "# print(\"Starting prediction on validation dataset...\")\n",
    "# for i in tqdm(range(len(dev))):\n",
    "#     y_pred.append(predict(dev[\"email_text\"][i], beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "590285a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute f1score\n",
    "from sklearn.metrics import f1_score\n",
    "# f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d924f5",
   "metadata": {},
   "source": [
    "## Task 2.3: Make prediction on test dataset¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae88a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rev_label_dict = {0: 'Democratic Party', 1: 'Republican Party'}\n",
    "\n",
    "# # make prediction\n",
    "# y_pred = []\n",
    "# print(\"Starting prediction on test dataset...\")\n",
    "# for i in tqdm(range(len(test))):\n",
    "#     y_pred.append(predict(test[\"email_text\"][i], beta))\n",
    "    \n",
    "# result = pd.DataFrame()\n",
    "# result[\"uid_email\"] = test[\"uid_email\"]\n",
    "# result[\"party_affiliation\"] = [rev_label_dict[p] for p in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e683e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output result\n",
    "# result.to_csv(\"part2_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e22df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe150710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f708e2bf",
   "metadata": {},
   "source": [
    "# Part 3: Logistic Regression with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c54ddf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore the installation command if torch has been installed\n",
    "# ! pip3 install torch torchvision torchaudio\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf49eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sparse_tensor(np_sparse_mat):\n",
    "    # INPUT: a numpy sprase matrix used previously\n",
    "    # OUTPUT: a torch sparse matrix\n",
    "    coo_mat = np_sparse_mat.tocoo()\n",
    "    \n",
    "    values = coo_mat.data\n",
    "    indices = np.vstack((coo_mat.row, coo_mat.col))\n",
    "    \n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = coo_mat.shape\n",
    "\n",
    "    torch_sparse_mat = torch.sparse_coo_tensor(i, v, torch.Size(shape))\n",
    "    return torch_sparse_mat\n",
    "\n",
    "train_mat_torch = to_sparse_tensor(train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2a51ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = nn.Sigmoid()\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_features = train_mat_torch.shape[1], output_features = 1, bias=True):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(input_features, output_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return sig(self.layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72e913fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useless helper function\n",
    "def GetTensorRow(tensor, i):\n",
    "    return tensor.index_select(0, torch.tensor([i % len(tensor)])).to_dense()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afa5f1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating doc list for dev ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:01<00:00, 19362.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating CSR sparse matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 20000/20000 [00:02<00:00, 8866.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# dev data preprocessing for prediction\n",
    "docs = []\n",
    "print(\"Generating doc list for dev ...\")\n",
    "for index, row in tqdm(dev.iterrows(), total = dev.shape[0]):\n",
    "    docs.append(tokenize(row[\"email_text\"]))\n",
    "    \n",
    "dev_mat, dev_vocabulary = CSRMatrixGeneration(docs, vocabulary=train_vocabulary)\n",
    "dev_mat = sparse.hstack([dev_mat, np.zeros(len(dev_mat.toarray()))[:,None]]) # add the missing column\n",
    "dev_mat = sparse.hstack([dev_mat, np.ones(len(dev_mat.toarray()))[:,None]]) # add the bias column\n",
    "dev_mat = sparse.csr_matrix(dev_mat)\n",
    "\n",
    "dev_mat_torch = to_sparse_tensor(dev_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0980092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a model, predict on the new text\n",
    "def predict(text, model, vocab_dict = train_vocabulary):\n",
    "    x_pred = np.zeros(len(vocab_dict) + 1)\n",
    "    \n",
    "    term_dict = Counter(tokenize(text))\n",
    "    for term, freq in term_dict.items():\n",
    "        if term in vocab_dict.keys():\n",
    "            x_pred[vocab_dict[term]] = freq\n",
    "    x_pred[-1] = 1 # bias\n",
    "    \n",
    "    y_pred = model(torch.tensor(x_pred, dtype=torch.float32))\n",
    "    if y_pred < 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# given a model, predict on the tensor of new text\n",
    "def better_predict(x_tensor, model):\n",
    "    y_pred = model(x_tensor)\n",
    "    if y_pred < 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8db594f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction and get f1 score\n",
    "dev_mat_torch_dense = dev_mat_torch.to_dense() # set global for quicklier running\n",
    "def GetF1Score(model, dev = dev):\n",
    "    y_test = [label_dict[p] for p in dev[\"party_affiliation\"]]\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(dev))):\n",
    "        y_pred.append(predict(dev[\"email_text\"][i], model))\n",
    "    return f1_score(y_test, y_pred) # compute f1score\n",
    "\n",
    "def better_GetF1Score(model, dev = dev):\n",
    "    # require dev_mat_torch as global variable\n",
    "    y_test = [label_dict[p] for p in dev[\"party_affiliation\"]]\n",
    "    y_pred = []\n",
    "    for i in range(len(dev_mat_torch)):\n",
    "        y_pred.append(\n",
    "            better_predict(dev_mat_torch_dense[i], model)\n",
    "        )\n",
    "    return f1_score(y_test, y_pred) # compute f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d484158",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat_torch_dense = train_mat_torch.to_dense()  # set global for quicklier running\n",
    "\n",
    "def TrainModel(X = train_mat_torch_dense, y = y_train, num_epoch = 1, num_step = len(train_mat_torch), \\\n",
    "               opt_choice = 1, learning_rate = 5e-5, l2penalty = 0, \\\n",
    "               isloss = False, isf1score = False):\n",
    "    # INPUT: X is a torch sparse matrix\n",
    "    # INPUT: y is a list or np.array of labels\n",
    "    # OUTPUT: a LR model with trained parameters\n",
    "    model = LogisticRegression()\n",
    "    criterion = nn.BCELoss() # loss function\n",
    "    if opt_choice == 1: # default optimizer\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=l2penalty)\n",
    "    elif opt_choice == 2:\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=l2penalty)\n",
    "    elif opt_choice == 3:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2penalty)\n",
    "\n",
    "    # for plotting\n",
    "    step_list = []\n",
    "    if isloss:\n",
    "        loss_list = []\n",
    "    if isf1score:\n",
    "        f1score_list = []\n",
    "\n",
    "    for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        print(f\"Starting training in epoch {epoch + 1}...\")\n",
    "        for i in tqdm(range(num_step)):\n",
    "            # get the inputs and label\n",
    "            inputs = X[i]\n",
    "            labels = torch.tensor(y_train[i], dtype=torch.float32).reshape(1)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                # print(f'epoch = {epoch + 1}, step = {i} => loss: {running_loss / 1000:.3f}')\n",
    "                step_list.append(epoch * num_step + i)\n",
    "                if isloss:\n",
    "                    loss_list.append(running_loss)\n",
    "                if isf1score:\n",
    "                    f1score_list.append(better_GetF1Score(model))\n",
    "                \n",
    "                running_loss = 0.0\n",
    "    \n",
    "    print('Finished Training')\n",
    "    \n",
    "    if isloss and isf1score: \n",
    "        return model, step_list, loss_list, f1score_list\n",
    "    elif isloss and not isf1score:\n",
    "        return model, step_list, loss_list\n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e24f17a",
   "metadata": {},
   "source": [
    "## Task 3.1: Compute loss and F1score for 5 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b0530",
   "metadata": {},
   "source": [
    "## Task 3.2: Compute loss and F1score for L2 penalty variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab462e",
   "metadata": {},
   "source": [
    "## Task 3.3: Compute loss and F1score for optimizer variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372d17cc",
   "metadata": {},
   "source": [
    "## Task 3.4: Compute loss and F1score for tokenization variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5bcde1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training in epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 59999/59999 [00:59<00:00, 1009.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# See additional file tokenize.ipynb\n",
    "tokenize_model1, step_list1, loss_list1, f1score_list1 = TrainModel(isloss=True, isf1score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8cfd46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to file for main file\n",
    "tokenize_data_df1 = pd.DataFrame({\n",
    "    \"step\": step_list1,\n",
    "    \"loss\": loss_list1,\n",
    "    \"f1score\": f1score_list1,\n",
    "    \"label\": \"Worse\"\n",
    "})\n",
    "\n",
    "tokenize_data_df1.to_csv(\"tokenize_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c4dbb1",
   "metadata": {},
   "source": [
    "## Task 3.5: Compute loss and F1score for learning rate variations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
